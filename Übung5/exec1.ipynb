{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import nltk\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "doc_dict = {}\n",
    "counter = 0\n",
    "\n",
    "for f in Path('/Users/robert/Documents/Uni/DataScience/Übung5').glob('*.txt'):\n",
    "    doc_dict.update({f.name:f.read_text('utf-8')})\n",
    "    counter+=1"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "for f_name in doc_dict:\n",
    "    doc_dict[f_name] = nltk.word_tokenize(doc_dict[f_name], language='german')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Calculate Word Frequencies per Document"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "bags_of_words = {}\n",
    "compl_bag_of_words = set()\n",
    "\n",
    "for f_name in doc_dict:\n",
    "    bags_of_words.update({f_name: {}})\n",
    "    for word in doc_dict[f_name]:\n",
    "        if word.lower() in bags_of_words[f_name]:\n",
    "            bags_of_words[f_name][word.lower()] = bags_of_words[f_name][word.lower()] + 1\n",
    "        else:\n",
    "            bags_of_words[f_name].update({word.lower(): 1})\n",
    "        compl_bag_of_words.add(word)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Now the cosine similarity (greatest will be clustered together in the next step)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "#length of vectors\n",
    "len_doc_vector = {}\n",
    "for f_name in bags_of_words:\n",
    "    length = 0\n",
    "    for word in bags_of_words[f_name]:\n",
    "        length += (bags_of_words[f_name][word])*(bags_of_words[f_name][word])\n",
    "    len_doc_vector.update({f_name: math.sqrt(length)})\n",
    "\n",
    "#dot product\n",
    "docs = list(bags_of_words.keys())\n",
    "dot_ps = {(docs[0], docs[1]): 0, (docs[0],docs[2]):0, (docs[1],docs[2]): 0}\n",
    "\n",
    "sims = {(0,1): {},(0,2): {},(1,2): {}}\n",
    "\n",
    "for word in compl_bag_of_words:\n",
    "    if word in bags_of_words[docs[0]] and word in bags_of_words[docs[1]]:\n",
    "        dot_ps[(docs[0],docs[1])] += bags_of_words[docs[0]][word]*bags_of_words[docs[1]][word]\n",
    "\n",
    "    if word in bags_of_words[docs[0]] and word in bags_of_words[docs[2]]:\n",
    "        dot_ps[(docs[0],docs[2])] += bags_of_words[docs[0]][word]*bags_of_words[docs[2]][word]\n",
    "\n",
    "    if word in bags_of_words[docs[1]] and word in bags_of_words[docs[2]]:\n",
    "        dot_ps[(docs[1],docs[2])] += bags_of_words[docs[1]][word]*bags_of_words[docs[2]][word]\n",
    "\n",
    "cos_sims = dot_ps.copy()\n",
    "\n",
    "for combination in cos_sims:\n",
    "    cos_sims[combination]  = cos_sims[combination] / (len_doc_vector[combination[0]] * len_doc_vector[combination[1]])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "data": {
      "text/plain": "{('erlkonig.txt', 'kollegah-universalgenie1strophe.txt'): 0.5256618065582226,\n ('erlkonig.txt', 'erlkoniganalyse.txt'): 0.5425230687931862,\n ('kollegah-universalgenie1strophe.txt',\n  'erlkoniganalyse.txt'): 0.5319882843361549}"
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cos_sims"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Euklidian distance"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "eucl_dist = {(docs[0], docs[1]): 0, (docs[0],docs[2]):0, (docs[1],docs[2]): 0}\n",
    "\n",
    "for combination in eucl_dist:\n",
    "    for word in compl_bag_of_words:\n",
    "        if word in bags_of_words[combination[0]] and word in bags_of_words[combination[1]]:\n",
    "            eucl_dist[combination] += math.pow(bags_of_words[combination[0]][word] - bags_of_words[combination[1]][word], 2)\n",
    "        elif word in bags_of_words[combination[0]]:\n",
    "            eucl_dist[combination] += math.pow(bags_of_words[combination[0]][word], 2)\n",
    "        elif word in bags_of_words[combination[1]]:\n",
    "            eucl_dist[combination] += math.pow(bags_of_words[combination[1]][word], 2)\n",
    "\n",
    "    eucl_dist[combination] = math.sqrt(eucl_dist[combination])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "data": {
      "text/plain": "{('erlkonig.txt', 'kollegah-universalgenie1strophe.txt'): 78.58116822750856,\n ('erlkonig.txt', 'erlkoniganalyse.txt'): 106.5926826756884,\n ('kollegah-universalgenie1strophe.txt',\n  'erlkoniganalyse.txt'): 105.75916035975324}"
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eucl_dist"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Begründung:\n",
    "\n",
    "An der euklidischen Distanz ist erkennbar, dass,\n",
    "wenn nun aus den elementaren Clustern (den Datenpunkten) ein zusammengefügtes Cluster nach Complete Link erstellt wird,\n",
    "zunächst ein Cluster aus erlkonig und kollegahs Universalgenie entsteht, während dies bei Cosine Similarity und Single Link stattdessen\n",
    "universalgenie (Strophe 1)  und erlkoniganalyse wären."
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
